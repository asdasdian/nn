# -*- coding: utf-8 -*-
"""tamrin_akhar_nn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XiH3bXq_ll51Ix9AegVlRaHqOfyx4i8T
"""

#Part 1
#import

from tensorflow.keras.datasets import fashion_mnist
from keras.datasets import mnist
from keras.callbacks import TensorBoard
import tensorflow.keras as keras
from tensorflow.keras import Sequential
from tensorflow.keras import layers
from tensorflow.keras.layers import InputLayer,Conv2D, Flatten, Dense,UpSampling2D
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import regularizers
from tensorflow.keras.layers import BatchNormalization

#Part 1
#classs


class AutoEncoder:
    def ___init(self):
        self.x_train=None
        self.x_test=None
        self.x_train_noisy=None
        self.x_test_noisy=None
        self.Model=None
        self.encode = None
        self.encoded_imgs = None
        self.predicted = None
        self.history = None

    def CreateNetwork(self):
        #Encoder Definition
        print("Creating Network...")
        input_img = layers.Input(shape=(784,))
        Encoder = layers.Dense(400, activation='relu')(input_img)
        Encoder = layers.Dense(100, activation='relu')(Encoder)
        LatentSpace = layers.Dense(65, activation='relu')(Encoder)




        # Decoder Definition
        Decoder = layers.Dense(100, activation='relu')(LatentSpace)
        Decoder = layers.Dense(400, activation='relu')(Decoder)
        decoded = layers.Dense(784, activation='relu')(Decoder)

        self.Model = keras.Model(input_img, decoded)
        self.encode = keras.Model(input_img, LatentSpace)
        self.Model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    def Train_With_noise(self,_epochs=20):
        print("Training Network")
        self.history= self.Model.fit(self.x_train_noisy, self.x_train,
                    epochs=_epochs,
                    batch_size=256,
                    shuffle=True,
                    validation_data=(self.x_test_noisy, self.x_test),
                    callbacks=[TensorBoard(log_dir='C://Users//alireza//Desktop//log', histogram_freq=0, write_graph=False)])

        self.encoded_imgs = self.encode.predict(self.x_test)
        self.predicted = self.Model.predict(self.x_test)

    def Train(self,_epochs=20):
        print("Training Network")
        self.history = self.Model.fit(self.x_train, self.x_train,
                    epochs=_epochs,
                    batch_size=256,
                    shuffle=True,
                    validation_data=(self.x_test, self.x_test),
                    callbacks=[TensorBoard(log_dir='C://Users//alireza//Desktop//log', histogram_freq=0, write_graph=False)])

        self.encoded_imgs = self.encode.predict(self.x_test)
        self.predicted = self.Model.predict(self.x_test)

    def PrepareDataset(self):
        (self.x_train, _), (self.x_test, _) = fashion_mnist.load_data()
        self.x_train = self.x_train.astype('float32') / 255.
        self.x_test = self.x_test.astype('float32') / 255.

        self.x_train = self.x_train.reshape((len(self.x_train), np.prod(self.x_train.shape[1:])))
        self.x_test = self.x_test.reshape((len(self.x_test), np.prod(self.x_test.shape[1:])))

    def Display(self):
        print("Showing Images")
        plt.figure(figsize = (40, 4))
        for i in range(10):
            #display original images
            ax = plt.subplot(3, 20, i+1)
            plt.imshow(self.x_test[i].reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)

            #disply encoded images
            ax = plt.subplot(3, 20, i+1+20)
            plt.imshow(self.encoded_imgs[i].reshape(13,5))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)

            #display reconstruction images
            ax = plt.subplot(3, 20 , 2*20+i+1)
            plt.imshow(self.predicted[i].reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
        plt.show()

    def FaultInject(self,noise_factor = 0.5):

        self.x_train_noisy = self.x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=self.x_train.shape)
        self.x_test_noisy = self.x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=self.x_test.shape)

        self.x_train_noisy = np.clip(self.x_train_noisy, 0., 1.)
        self.x_test_noisy = np.clip(self.x_test_noisy, 0., 1.)

    def evaluation(self):
        plt.figure(figsize=(14,5))
        plt.subplot(1,2,1)
        plt.plot(self.history.history['loss'])
        plt.plot(self.history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Test'], loc='upper left')

        plt.subplot(1,2,2)
        plt.plot(self.history.history['accuracy'])
        plt.plot(self.history.history['val_accuracy'])
        plt.title('Model Accuracy')
        plt.ylabel('Accuracy')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Test'], loc='upper left')
        plt.show()
        print(f'Training Loss: {self.history.history["loss"][-1]}')
        print(f'Training Accuracy: {self.history.history["accuracy"][-1]}')
        print(f'Validation Loss: {self.history.history["val_loss"][-1]}')
        print(f'Validation Accuracy: {self.history.history["val_accuracy"][-1]}')

    def CreateNetwork_with_batch_regul(self):
        print("Creating Network with Batch Normalization and Regularization...")
        input_img = layers.Input(shape=(784,))
        Encoder = layers.Dense(400, activation='relu', kernel_regularizer=regularizers.l2(0.01))(input_img)
        Encoder = BatchNormalization()(Encoder)
        Encoder = layers.Dense(150, activation='relu', kernel_regularizer=regularizers.l2(0.01))(Encoder)
        Encoder = BatchNormalization()(Encoder)
        LatentSpace = layers.Dense(65, activation='relu', kernel_regularizer=regularizers.l2(0.01))(Encoder)#buttle neck layer
        LatentSpace = BatchNormalization()(LatentSpace)

        Decoder = layers.Dense(150, activation='relu', kernel_regularizer=regularizers.l2(0.01))(LatentSpace)
        Decoder = BatchNormalization()(Decoder)
        Decoder = layers.Dense(400, activation='relu', kernel_regularizer=regularizers.l2(0.01))(Decoder)
        Decoder = BatchNormalization()(Decoder)
        decoded = layers.Dense(784, activation='relu', kernel_regularizer=regularizers.l2(0.01))(Decoder)
        decoded = BatchNormalization()(decoded)





        self.Model = keras.Model(input_img, decoded)
        self.encode = keras.Model(input_img, LatentSpace)
        self.Model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#Part 2

AutoEnc = AutoEncoder()
AutoEnc.PrepareDataset()
AutoEnc.CreateNetwork()
AutoEnc.Train(30)
AutoEnc.evaluation()
AutoEnc.Display()



AutoEnc_wbath = AutoEncoder()
AutoEnc_wbath.PrepareDataset()
AutoEnc_wbath.CreateNetwork_with_batch_regul()
AutoEnc_wbath.Train(30)
AutoEnc_wbath.evaluation()
AutoEnc_wbath.Display()

###############
###############
###############
###############
###############

#Part3
#import


from tensorflow.keras.datasets import fashion_mnist
from keras.datasets import mnist
from keras.callbacks import TensorBoard
import tensorflow.keras as keras
from tensorflow.keras import Sequential
from tensorflow.keras import layers
from tensorflow.keras.layers import InputLayer,Conv2D, Flatten, Dense,UpSampling2D
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import regularizers
from tensorflow.keras.layers import BatchNormalization

#Part3
#class


class AutoEncoder:
    def ___init(self):
        self.x_train=None
        self.x_test=None
        self.x_train_noisy=None
        self.x_test_noisy=None
        self.Model=None
        self.encode = None
        self.encoded_imgs = None
        self.predicted = None
        self.history = None

    def CreateNetwork(self):
        #Encoder Definition
        print("Creating Network...")
        input_img = layers.Input(shape=(784,))
        Encoder = layers.Dense(400, activation='relu')(input_img)
        Encoder = layers.Dense(150, activation='relu')(Encoder)
        LatentSpace = layers.Dense(65, activation='relu')(Encoder)#buttle neck layer

        # Decoder Definition

        Decoder = layers.Dense(150, activation='relu')(LatentSpace)
        Decoder = layers.Dense(400, activation='relu')(Decoder)
        decoded = layers.Dense(784, activation='relu')(Decoder)

        self.Model = keras.Model(input_img, decoded)
        self.encode = keras.Model(input_img, LatentSpace)
        self.Model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    def Train_With_noise(self,_epochs=20):
        print("Training Network")
        self.history= self.Model.fit(self.x_train_noisy, self.x_train,
                    epochs=_epochs,
                    batch_size=128,
                    shuffle=True,
                    validation_data=(self.x_test_noisy, self.x_test),
                    callbacks=[TensorBoard(log_dir='C://Users//alireza//Desktop//FTS_CA2', histogram_freq=0, write_graph=False)])

        self.encoded_imgs = self.encode.predict(self.x_test)
        self.predicted = self.Model.predict(self.x_test)

    def Train(self,_epochs=20):
        print("Training Network")
        self.history = self.Model.fit(self.x_train, self.x_train,
                    epochs=_epochs,
                    batch_size=128,
                    shuffle=True,
                    validation_data=(self.x_test, self.x_test),
                    callbacks=[TensorBoard(log_dir='C://Users//alireza//Desktop//FTS_CA2', histogram_freq=0, write_graph=False)])

        self.encoded_imgs = self.encode.predict(self.x_test)
        self.predicted = self.Model.predict(self.x_test)

    def PrepareDataset(self):
        (self.x_train, _), (self.x_test, _) = fashion_mnist.load_data()
        self.x_train = self.x_train.astype('float32') / 255.
        self.x_test = self.x_test.astype('float32') / 255.

        self.x_train = self.x_train.reshape((len(self.x_train), np.prod(self.x_train.shape[1:])))
        self.x_test = self.x_test.reshape((len(self.x_test), np.prod(self.x_test.shape[1:])))

    def Display(self):
        print("Showing Images")
        plt.figure(figsize = (40, 4))
        for i in range(10):
            #display original images
            ax = plt.subplot(4, 20, i+1)
            plt.imshow(self.x_test[i].reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)

            #display noised images
            ax = plt.subplot(4, 20, i+1+20)
            plt.imshow(self.x_test_noisy[i].reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_xaxis().set_visible(False)


            #disply encoded images
            ax = plt.subplot(4, 20, 2*20+i+1)
            plt.imshow(self.encoded_imgs[i].reshape(5,13))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)

            #display reconstruction images
            ax = plt.subplot(4, 20 , 3*20+i+1)
            plt.imshow(self.predicted[i].reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
        plt.show()
    def FaultInject(self,noise_factor = 0.5):

        self.x_train_noisy = self.x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=self.x_train.shape)
        self.x_test_noisy = self.x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=self.x_test.shape)

        self.x_train_noisy = np.clip(self.x_train_noisy, 0., 1.)
        self.x_test_noisy = np.clip(self.x_test_noisy, 0., 1.)

    def evaluation(self):
        plt.figure(figsize=(14,5))
        plt.subplot(1,2,1)
        plt.plot(self.history.history['loss'])
        plt.plot(self.history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Test'], loc='upper left')

        plt.subplot(1,2,2)
        plt.plot(self.history.history['accuracy'])
        plt.plot(self.history.history['val_accuracy'])
        plt.title('Model Accuracy')
        plt.ylabel('Accuracy')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Test'], loc='upper left')
        plt.show()
        print(f'Training Loss: {self.history.history["loss"][-1]}')
        print(f'Training Accuracy: {self.history.history["accuracy"][-1]}')
        print(f'Validation Loss: {self.history.history["val_loss"][-1]}')
        print(f'Validation Accuracy: {self.history.history["val_accuracy"][-1]}')

    def CreateNetwork_with_batch_regul(self):
        print("Creating Network with Batch Normalization and Regularization...")
        input_img = layers.Input(shape=(784,))
        Encoder = layers.Dense(400, activation='relu', kernel_regularizer=regularizers.l2(0.01))(input_img)
        Encoder = BatchNormalization()(Encoder)
        Encoder = layers.Dense(300, activation='relu', kernel_regularizer=regularizers.l2(0.01))(Encoder)
        Encoder = BatchNormalization()(Encoder)
        Encoder = layers.Dense(150, activation='relu', kernel_regularizer=regularizers.l2(0.01))(Encoder)
        Encoder = BatchNormalization()(Encoder)
        LatentSpace = layers.Dense(65, activation='relu', kernel_regularizer=regularizers.l2(0.01))(Encoder)#buttle neck layer
        LatentSpace = BatchNormalization()(LatentSpace)

        Decoder = layers.Dense(150, activation='relu', kernel_regularizer=regularizers.l2(0.01))(LatentSpace)
        Decoder = BatchNormalization()(Decoder)
        Decoder = layers.Dense(300, activation='relu', kernel_regularizer=regularizers.l2(0.01))(Decoder)
        Decoder = BatchNormalization()(Decoder)
        Decoder = layers.Dense(400, activation='relu', kernel_regularizer=regularizers.l2(0.01))(Decoder)
        Decoder = BatchNormalization()(Decoder)
        decoded = layers.Dense(784, activation='relu', kernel_regularizer=regularizers.l2(0.01))(Decoder)
        decoded = BatchNormalization()(decoded)

        self.Model = keras.Model(input_img, decoded)
        self.encode = keras.Model(input_img, LatentSpace)
        self.Model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#Part 3

AutoEnc = AutoEncoder()
AutoEnc.PrepareDataset()
AutoEnc.CreateNetwork()
AutoEnc.FaultInject(0.5)
AutoEnc.Train_With_noise(30)
AutoEnc.evaluation()
AutoEnc.Display()

#Part4
#import
import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.layers import Dense, Lambda, Input, Flatten, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.losses import binary_crossentropy, SparseCategoricalCrossentropy
from tensorflow.keras import backend as K
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt

#Part4
#class

class VariationalAutoencoder:
    def __init__(self, latent_dim=2, batch_size=100, num_classes=10):
        self.latent_dim = latent_dim
        self.batch_size = batch_size
        self.num_classes = num_classes
        self.autoencoder = None
        self.encoder = None
        self.decoder = None
        self.classifier = None
        self.x_train = None
        self.y_train = None
        self.x_test = None
        self.y_test = None
        self.x_val = None
        self.y_val = None
    def prepare_dataset(self):
        (self.x_train, self.y_train), (self.x_test, self.y_test) = fashion_mnist.load_data()
        self.x_train = self.x_train.astype('float32') / 255.
        self.x_test = self.x_test.astype('float32') / 255.
        self.x_train = self.x_train.reshape((len(self.x_train), np.prod(self.x_train.shape[1:])))
        self.x_test = self.x_test.reshape((len(self.x_test), np.prod(self.x_test.shape[1:])))

    def sampling(self, args):
        z_mean, z_log_var = args
        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.latent_dim), mean=0., stddev=0.1)
        return z_mean + K.exp(z_log_var / 2) * epsilon

    def build_and_compile_autoencoder(self, input_shape=(784,)):
        # Encoder
        inputs = Input(shape=input_shape, name='encoder_input')
        x = Dense(512, activation='relu')(inputs)
        z_mean = Dense(self.latent_dim, name='z_mean')(x)
        z_log_var = Dense(self.latent_dim, name='z_log_var')(x)
        z = Lambda(self.sampling, output_shape=(self.latent_dim,), name='z')([z_mean, z_log_var])
        self.encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')

        # Decoder
        latent_inputs = Input(shape=(self.latent_dim,), name='z_sampling')
        x = Dense(512, activation='relu')(latent_inputs)
        outputs = Dense(input_shape[0], activation='sigmoid')(x)
        self.decoder = Model(latent_inputs, outputs, name='decoder')

        # VAE
        outputs = self.decoder(self.encoder(inputs)[2])
        self.autoencoder = Model(inputs, outputs, name='vae')

        # Loss
        reconstruction_loss = binary_crossentropy(inputs, outputs) * input_shape[0]
        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
        kl_loss = K.sum(kl_loss, axis=-1) * -0.5
        vae_loss = K.mean(reconstruction_loss + kl_loss)
        self.autoencoder.add_loss(vae_loss)
        self.autoencoder.compile(optimizer='adam')

    def build_and_compile_classifier(self, input_shape=(2,)):
        # Classifier
        inputs = Input(shape=input_shape, name='classifier_input')
        x = Dense(512, activation='relu')(inputs)
        outputs = Dense(self.num_classes, activation='softmax')(x)
        self.classifier = Model(inputs, outputs, name='classifier')
        self.classifier.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])

    def train_autoencoder(self, epochs=100):
        self.autoencoder.fit(self.x_train, epochs=epochs, batch_size=self.batch_size, validation_data=(self.x_test, None))

    def train_classifier(self, epochs=100):
        x_train_encoded = vae.encoder.predict(self.x_train)[2]
        x_test_encoded = vae.encoder.predict(self.x_test)[2]
        self.classifier.fit(x_train_encoded, self.y_train, epochs=epochs, batch_size=self.batch_size, validation_data=(x_test_encoded, self.y_test))

    def generate_and_print_images(self):
        z_sample = np.array([[xi, yi] for xi in np.linspace(-3, 3, 10) for yi in np.linspace(-3, 3, 10)])
        x_decoded = self.decoder.predict(z_sample)

        # Print the first 10 images
        for i, image in enumerate(x_decoded[:10]):
            plt.subplot(2, 5, i+1)
            plt.imshow(image.reshape(28, 28), cmap='gray')
        plt.show()

vae = VariationalAutoencoder()
vae.prepare_dataset() # prepare the dataset before training
vae.build_and_compile_autoencoder()
vae.train_autoencoder(epochs=30)

# Build and train the classifier
vae.build_and_compile_classifier()
vae.train_classifier(epochs=30)

# Generate and print images
vae.generate_and_print_images()